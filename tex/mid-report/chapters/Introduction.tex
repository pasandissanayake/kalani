\chapter{Introduction}
\glsresetall

% TODO: overview

%%%%%%%%%%%%%%%%
\section{Problem definition and scope}
The amount of autonomy in vehicles is divided into six levels and vehicles beyond level three are considered as self-driving. Such vehicles are expected to travel from a starting point to a given destination, with minimal human-driver involvement. Therefore, they need to know their location with a very high accuracy, relative to their immediate environment as well as in a global level. Other than the location itself, it is important to provide details of other state variables of the vehicle such as the speed and the orientation, which will be used by top-level controlling algorithms. This information should be updated uninterruptedly and frequently to preserve accuracy under higher speeds, which is the job of a localization module. The most widely used mechanism for fulfilling this requirement is, fusing data obtained from different sensors such as \gls{GNSS}, \gls{LiDAR}, \gls{RADAR} and cameras to obtain the most probable state using a Bayesian filter. This is known as sensor fusion.

As we have noticed, a main limitation of the existing state-of-the-art work in this regard is the dependency of these solutions on different kinds of existing, accurate feature maps. These maps are used as inputs to the localization module, which reduces the scalability of the solution due to the fact that creating, updating and storing such highly-detailed maps of an entire region or a country is not so feasible. We also note the absence of a detailed workflow describing complete implementation of a localization module, which in-turn wastes the time and effort of the research community, by having to start from the beginning, all the time.

Therefore, it is the aim of this project to implement a localization module which addresses problems mentioned above, while providing enough accuracy and update frequency to allow self-driving. Mentioning specifically, a sub-meter level positional accuracy is targeted along with a frequency of 70 Hz or more, which will be sufficient for speeds below 50 km h\textsuperscript{-1}\cite{pa:ComputerArchitectures}. As this work is a part of the top-level project aiming the construction of a fully autonomous vehicle, the system is implemented on Robot Operating System (ROS) to facilitate easy integration with other modules. The solution will initially be tuned and tested using freely available datasets and, eventually, it will be tested using actual sensors. The accuracy will only be evaluated using datasets, by comparing with the provided ground truth data.

While self-driving is itself a novel concept in the Sri Lankan context, this project aims in resolving the dependency of state-of-the-art work on feature maps, thereby allowing accurate localization in unstructured, constantly changing environments. We intend to achieve this goal mainly through improving the data fusion algorithm. Other than the field of self-driving itself, we expect that this mechanism will be useful in different applications such as robotics navigation and navigational equipment development. The project is carried out with the collaboration of Creative Software Private Limited.





%%%%%%%%%%%%%%%%
\section{Related work}
Localization using multi-sensor fusion is not a brand-new idea. A lot of researches have been done in this area for the past decade. The self-driving car concept was firstly addressed with \gls{M-ELROB} and \gls{DARPA} Urban Challenge competitions in 2006 and 2007\cite{pa:GIZA}. The SmartTer \cite{pa:Smartter} and Stanford Junior \cite{pa:StanfordJunior} are two self-driving car projects who won these competitions. Google’s car \cite{pa:GoogleCar}, VisLab’s Car \cite{pa:VisilabCar}, Apollo \cite{pa:Apollo} and Autoware Auto \cite{pa:AutowareAuto} are some examples of successful research projects. However, self-driving cars are still in the introductory phase of the product life cycle.

Normally, \gls{GNSS}, \gls{IMU}, \gls{LiDAR}, \gls{RADAR} and wheel encoders are used to localize a robot. The combination of sensors depends on the design. Different sensors have different shortcomings. \gls{GNSS} signal may not be available on a covered area, underground, or in a tunnel. Normally \gls{GNSS} accuracy is about 10m due to satellite orbit and clock errors \cite{pa:Wan2018}. In addition, when the vehicle drives next to large structures, \gls{GNSS} measurement can go wrong due to reflections \cite{pa:Smartter}. GISA, which is a Brazilian platform for autonomous car trials uses \gls{DGPS} as one of the sensors \cite{pa:GIZA}. It gives better accuracy than normal \gls{GNSS}. \gls{DGPS} has also been used by the SmartTer. But standard \gls{GNSS} is available more often when compared to \gls{DGPS} because it does not rely on the visibility of geostationary satellites which provide the \gls{DGPS} corrections \cite{pa:Smartter}. The enhanced \gls{GNSS} technique known as \gls{RTK} has been used by Wan et al. in their self-driving car for localization \cite{pa:Wan2018}. However, this method is also prone to significant errors caused by multipath effects and signal blockages due to its dependency on precision carrier-phase positioning techniques. It is noted that most of the projects use normal \gls{GNSS}\cite{pa:StanfordJunior} \cite{pa:Soloviev2008}\cite{pa:Levinson2010}\cite{pa:Roumeliotis1999}. \gls{LiDAR} works well when the environment is full of 3D or texture features, but it fails in open spaces \cite{pa:Wan2018}. 3D \gls{LiDAR} sensors were used by GIZA\cite{pa:GIZA}, Wan et al. \cite{pa:Wan2018} and Levinson et al. \cite{pa:Levinson2011}, while 2D \gls{LiDAR} sensors were used by Soloviev \cite{pa:Soloviev2008}, \cite{pa:Soloviev2007}  and Baldwin and Newman \cite{pa:Baldwin2012}. Erik Ward and John Folkesson used \gls{RADAR} as the measurement model input as mentioned in \cite{pa:Ward2016}. Both \gls{LiDAR} and \gls{RADAR} sensors have the same kind of behavior when they act as measurement model inputs. \gls{IMU} sensors have been used in almost every project. However, it suffers from the accumulation of integration errors \cite{pa:Wan2018}. Wheel encoders have been used along with \gls{IMU} in \cite{pa:Smartter}, but it provides incorrect measurements when wheels slip. As mentioned above, each sensor has its own drawbacks and advantages. Other than these sensors Wan et al. \cite{pa:Wan2018}, Levinson and Thrun \cite{pa:Levinson2010}, Stanford Junior \cite{pa:StanfordJunior}, and Apollo \cite{pa:Apollo} projects have used a pre-built map. Currently, some measurement companies have already begun to prepare map databases for self-driving vehicles \cite{pa:Yoneda2014}.

In the works of Wan et al., they have used \gls{LiDAR} intensity and altitude cues with 3D geometry for their \gls{LiDAR} based localization module. They have obtained a grid-cell representation of the environment using a single Gaussian distribution to model the environment which involved both the intensity and the altitude. Finally, an Error-State Kalman filter was applied to fuse the data from the sensors. They have achieved 0.05-0.1 m \gls{RMS} accuracy in both longitudinal and lateral directions \cite{pa:Wan2018}. The Stanford Junior which won the second place of DARPA Urban Challenge competition was given a digital map of the road network in the form of a \gls{RNDF}. The \gls{RNDF} contained geometric information about lane markings, stop signs, parking lots and special checkpoints. They were specified in \gls{GNSS} coordinates. Local alignment between the \gls{RNDF} and the vehicle’s current position was estimated using \gls{GNSS} along with two laser sensor measurements \cite{pa:StanfordJunior}. After the competition, they have upgraded the ground map using \gls{GNSS}, \gls{IMU} and Velodyne \gls{LiDAR} data. Here, every cell was represented as its own Gaussian distribution. By this method, they have achieved a lateral \gls{RMS} accuracy better than 0.1 m. Levinson and Thrun have localized the vehicle using a probabilistic map. They have modeled the environment as a probabilistic grid whereby every cell is represented as its own Gaussian distribution over remittance values. Furthermore, offline \gls{SLAM} was used to align multiple passes of the same environment. Once a map had been built, they have used it to localize the vehicle in real time by representing the likelihood distribution of possible x and y offsets with a 2-dimensional histogram filter. The resulting error after localization has been extremely low, with an \gls{RMS} value of 0.09 m \cite{pa:Levinson2010}. A hybrid model with both Kalman filter and particle filter has been proposed by Won et al. in \cite{pa:Won2010} as the sensor fusion algorithm. They have used the particle filter to estimate the orientation and the Kalman filter for estimating the position and velocity.
As per the above discussion, it is clear that the most widely used and successful method for localization for self-driving is fusing data obtained from different sensors. These sensors should be selected carefully, so that they have complementary properties and functioning capabilities under different environmental conditions. As an example, a \gls{GNSS} receiver may function well in an open environment, in which a \gls{LiDAR} is of very little use. Conversely, a \gls{LiDAR} can give a very detailed output while in an urban environment, in which a \gls{GNSS} receiver may fail due to multipath effects and signal blockage. We also note that in almost all the works, a Bayesian filter (Extended Kalman, Particle filter etc.) or a combination of many, has been used as the data fusion algorithm. 

Another important deduction that can be made from the above comparison is that most of the state-of-the-art work depends on the assumption of an existing accurate map, which is given as an input to the localization mechanism. Even though this dependency is satisfiable under constrained environments such as competitions, it reduces the scalability of the solution drastically when it comes to unstructured environments, which a self-driving vehicle will be experiencing most frequently under normal operation. This is also the case for countries like Sri Lanka where highly detailed mapping of the entire country is a tedious task due to resource constrains and frequent and unplanned changings of the features. Hence, we find it is extremely important to focus on reaping the maximum localization accuracy possible, from a given combination of sensors.





%%%%%%%%%%%%%%
\section{Method of investigation and results}
After a comprehensive study of literature pertaining to current developments, we decided to use a Bayesian framework of sensor fusion, along with the sensor data from (not limited to) \gls{GNSS} receiver, \gls{IMU}, \gls{LiDAR}, stereo camera pair, magnetometer and wheel odometer. In-order to decide upon the type of the Bayesian filter to be used, several such filters with a minimal complexity were implemented, and accuracies were compared using data from existing datasets. As developing visual odometry (from stereo pair of cameras) and \gls{LiDAR} odometry algorithms were out of the scope of the project, existing state-of-the-art algorithms were used with minor modifications. A filter framework with stochastic cloning and backward smoothing functionality was implemented in Python. Special attention was given to implement this framework in an easily modifiable manner (mainly, the motion model and state variables), so that, it facilitates experimenting with different filter structures. Output of the framework is compared with the ground truth provided in the datasets to obtain accuracy measures. As per the current state of the project, two main problems have been identified; mechanism to estimate an error covariance matrix for visual/\gls{LiDAR} odometry algorithms and treating the correlated noise of the \gls{GNSS} measurements. A detailed discussion of the method of investigation and the results obtained will be carried out in the subsequent chapters.